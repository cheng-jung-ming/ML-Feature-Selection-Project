# ğŸ¯ Feature selection and AI explainability

> Used on the Kaggle website, public data sets: feature selection and interpretable machine learning projects for customer churn prediction and house price prediction.  
> âœ¨ Integrated model insights from SHAP, PFI, RF importance, FK-RFE, and conditional PFI.

---

## ğŸ“Œ Overview

This project aims to build an interpretable and accurate machine learning pipeline. We do not rely on a single feature selection method, but instead combine multiple methods to ensure robust and interpretable feature importance.

---

## ğŸ§  Core Concepts

- ğŸ”**Integrated Feature Importance**: Combines SHAP, PFI, and RF's internal Gini-based feature importance to avoid single model bias.
- ğŸŒ€ **FK-RFE**: A hybrid feature selection pipeline combining Kolmogorov filter and recursive elimination via random forest.
- ğŸŒ¿ **Conditional PFI**: Improves traditional PFI (implemented in RF) by conditioning on local subgroups derived from decision tree paths.
- âš™ï¸ **Model Benchmarking**: Random Forest (ML), TabNet (DL) â€” comparing performance and interpretability.

---

## ğŸ—ï¸ Project Structure

ğŸ’¡ Key Techniques Used

Permutation Feature Importance (PFI)

SHAP (Shapley Additive Explanations)

Random Forest Gini Importance

FK-RFE (Kolmogorov Filter + RF-RFE)

Conditional Permutation Feature Importance (cPFI)

TabNet Deep Feature Learning

---
## ğŸ“ˆ Model Performance
![Performance Plot](images-Performance/æ•ˆèƒ½åœ–ç¤º.png)

## ğŸ¤–ğŸ” AI Explainability
![Performance Plot](images-Performance/waterfull_plot.png)

## ğŸ“š References

Fisher, A., Rudin, C., & Dominici, F. (2019). All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1â€“81.

Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 30.

Arik, S. Ã–., & Pfister, T. (2021). TabNet: Attentive interpretable tabular learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8), 6679â€“6687.

Zhang, Y., Zhu, Z., Liu, X., & Yang, W. (2023). A Modelâ€Free Feature Selection Technique of Feature Screening and Random Forestâ€Based Recursive Feature Elimination. Expert Systems, 40(2), e13034.

Pfisterer, F., & Biecek, P. (2024). Model-agnostic feature importance and effects with dependent features: A conditional subgroup approach. Journal of Machine Learning Research, 25(25), 1â€“45.

## ğŸ™Œ Acknowledgements

Special thanks to National Chung Hsing University and the professors who guided this project.

## ğŸ“® Contact

ğŸ“§ [ccit0915@gmail.com]
ğŸ« Department of Applied Mathematics, National Chung Hsing University
